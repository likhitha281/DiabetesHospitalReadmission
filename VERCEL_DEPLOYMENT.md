# Vercel Deployment Guide for Cluster Prediction Dashboard

This guide explains how to deploy your interactive cluster prediction dashboard on Vercel.

## Prerequisites

1. **Run the notebook** to generate model files:
   - Execute all cells in `ClusteringAndDimensionalityReduction.ipynb`
   - The last cell will save models to the `models/` directory

2. **Install dependencies**:
   ```bash
   npm install
   ```

## Project Structure

```
DiabetesHospitalReadmission/
├── api/
│   ├── predict.py          # Python serverless function
│   └── requirements.txt     # Python dependencies
├── pages/
│   ├── index.js           # Next.js frontend
│   └── api/
│       └── predict.js     # Next.js API route (proxy)
├── models/                # Saved models (generated by notebook)
│   ├── scaler.pkl
│   ├── label_encoders.pkl
│   ├── kmeans_model.pkl
│   ├── feature_info.pkl
│   └── cluster_profiles.pkl
├── vercel.json            # Vercel configuration
└── package.json           # Node.js dependencies
```

## Important Notes

### Model File Size
Vercel serverless functions have a **50MB limit** for the deployment package. If your model files are too large:

1. **Option 1: Use external storage** (Recommended for large models)
   - Upload models to AWS S3, Google Cloud Storage, or similar
   - Modify `api/predict.py` to download models on first request
   - Cache models in memory

2. **Option 2: Optimize models**
   - Use model compression techniques
   - Consider using smaller model variants
   - Remove unnecessary data from pickle files

3. **Option 3: Use alternative hosting**
   - Deploy Python API separately (Railway, Render, etc.)
   - Update frontend to call external API

### Python Runtime
Vercel supports Python 3.9+ for serverless functions. The `api/predict.py` file will automatically use Vercel's Python runtime.

## Deployment Steps

### 1. Prepare Your Project

Ensure all files are in place:
- ✅ Models generated in `models/` directory
- ✅ `api/predict.py` exists
- ✅ `api/requirements.txt` exists
- ✅ `vercel.json` is configured
- ✅ `package.json` has dependencies

### 2. Deploy to Vercel

#### Option A: Using Vercel CLI

```bash
# Install Vercel CLI
npm i -g vercel

# Login to Vercel
vercel login

# Deploy
vercel

# For production
vercel --prod
```

#### Option B: Using GitHub Integration

1. Push your code to GitHub
2. Go to [vercel.com](https://vercel.com)
3. Click "New Project"
4. Import your GitHub repository
5. Vercel will auto-detect Next.js and configure it
6. Add environment variables if needed
7. Click "Deploy"

### 3. Verify Deployment

After deployment:
1. Visit your Vercel URL (e.g., `https://your-project.vercel.app`)
2. Test the dashboard by entering patient information
3. Check function logs in Vercel dashboard if there are issues

## Troubleshooting

### Issue: "Module not found" or Import Errors

**Solution**: Ensure `api/requirements.txt` includes all necessary packages:
```
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
```

### Issue: "Model files not found"

**Solution**: 
- Ensure models are committed to Git (or use external storage)
- Check that `models/` directory is in the project root
- Verify file paths in `api/predict.py`

### Issue: Function timeout or memory errors

**Solution**:
- Models might be too large
- Consider using external storage for models
- Optimize model loading (cache in global variable)

### Issue: CORS errors

**Solution**: The Python function already includes CORS headers. If issues persist, check:
- `Access-Control-Allow-Origin` headers in `api/predict.py`
- Next.js API route CORS settings

### Issue: "Function size exceeds limit"

**Solution**: 
- Use external storage for models (S3, etc.)
- Compress models before deployment
- Consider splitting into multiple functions

## Alternative: External Python API

If Vercel's limitations are an issue, you can:

1. **Deploy Python API separately**:
   - Use Railway, Render, or similar
   - Deploy a Flask/FastAPI app with your models
   - Update `pages/api/predict.js` to call external API

2. **Example Flask API** (deploy separately):
   ```python
   from flask import Flask, request, jsonify
   from flask_cors import CORS
   # ... (use same prediction logic)
   
   app = Flask(__name__)
   CORS(app)
   
   @app.route('/predict', methods=['POST'])
   def predict():
       # Same logic as api/predict.py
       pass
   ```

## Environment Variables (Optional)

If using external storage, add in Vercel dashboard:
- `AWS_ACCESS_KEY_ID` (if using S3)
- `AWS_SECRET_ACCESS_KEY` (if using S3)
- `MODEL_STORAGE_URL` (if using custom storage)

## Testing Locally

Before deploying:

```bash
# Install dependencies
npm install

# Run Next.js dev server
npm run dev

# Test the API endpoint
curl -X POST http://localhost:3000/api/predict \
  -H "Content-Type: application/json" \
  -d '{"data": {...}}'
```

## Support

For Vercel-specific issues:
- [Vercel Python Documentation](https://vercel.com/docs/concepts/functions/serverless-functions/runtimes/python)
- [Vercel Community](https://github.com/vercel/vercel/discussions)

For model-related issues:
- Check that notebook ran successfully
- Verify model files exist and are not corrupted
- Test model loading locally first

