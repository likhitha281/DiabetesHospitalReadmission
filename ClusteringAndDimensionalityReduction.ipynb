{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "487ac735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comprehensive Clustering and Dimensionality Reduction Analysis\n",
    "Hospital Readmission Project - Track 5\n",
    "\n",
    "Includes:\n",
    "- K-Means, Hierarchical, DBSCAN clustering\n",
    "- PCA, t-SNE, UMAP dimensionality reduction\n",
    "- Cluster evaluation and profiling\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, \n",
    "    davies_bouldin_score, \n",
    "    calinski_harabasz_score,\n",
    "    silhouette_samples\n",
    ")\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba335ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import UMAP (optional)\n",
    "try:\n",
    "    from umap import UMAP\n",
    "    UMAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    UMAP_AVAILABLE = False\n",
    "    print(\"âš ï¸  UMAP not available. Install with: pip install umap-learn\")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ac6a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING DATA\n",
      "============================================================\n",
      "Dataset shape: (101766, 50)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1. LOAD DATA\n",
    "# =========================\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = pd.read_csv(\"data/diabetic_data.csv\")\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb253b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE SELECTION FOR CLUSTERING\n",
      "============================================================\n",
      "Selected 26 features\n",
      "  - Numeric: 8\n",
      "  - Categorical: 9\n",
      "  - Medications: 9\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2. FEATURE SELECTION\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE SELECTION FOR CLUSTERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Focus on features that define patient profiles\n",
    "numeric_features = [\n",
    "    \"time_in_hospital\",\n",
    "    \"num_lab_procedures\",\n",
    "    \"num_procedures\",\n",
    "    \"num_medications\",\n",
    "    \"number_outpatient\",\n",
    "    \"number_emergency\",\n",
    "    \"number_inpatient\",\n",
    "    \"number_diagnoses\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"age\",\n",
    "    \"admission_type_id\",\n",
    "    \"discharge_disposition_id\",\n",
    "    \"admission_source_id\",\n",
    "    \"max_glu_serum\",\n",
    "    \"A1Cresult\",\n",
    "    \"diabetesMed\"\n",
    "]\n",
    "\n",
    "# Optional: medication features\n",
    "medication_features = [\n",
    "    \"metformin\", \"repaglinide\", \"nateglinide\", \n",
    "    \"glimepiride\", \"glipizide\", \"glyburide\", \n",
    "    \"pioglitazone\", \"rosiglitazone\", \"insulin\"\n",
    "]\n",
    "\n",
    "all_features = numeric_features + categorical_features + medication_features\n",
    "\n",
    "print(f\"Selected {len(all_features)} features\")\n",
    "print(f\"  - Numeric: {len(numeric_features)}\")\n",
    "print(f\"  - Categorical: {len(categorical_features)}\")\n",
    "print(f\"  - Medications: {len(medication_features)}\")\n",
    "\n",
    "# Create working dataframe\n",
    "df_cluster = df[all_features + ['readmitted', 'encounter_id']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19bbe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PREPROCESSING\n",
      "============================================================\n",
      "Total missing values: 0\n",
      "Final dataset size: (298, 28)\n",
      "\n",
      "============================================================\n",
      "ENCODING FEATURES\n",
      "============================================================\n",
      "Combined feature matrix: (298, 26)\n",
      "\n",
      "============================================================\n",
      "SCALING FEATURES\n",
      "============================================================\n",
      "Scaled data shape: (298, 26)\n",
      "\n",
      "============================================================\n",
      "FINDING OPTIMAL NUMBER OF CLUSTERS\n",
      "============================================================\n",
      "\n",
      "Evaluating different k values...\n",
      "k=2: Silhouette=0.185, Davies-Bouldin=2.649, Calinski-Harabasz=25.4\n",
      "k=3: Silhouette=0.073, Davies-Bouldin=3.179, Calinski-Harabasz=21.7\n",
      "k=4: Silhouette=0.067, Davies-Bouldin=2.797, Calinski-Harabasz=19.8\n",
      "k=5: Silhouette=0.077, Davies-Bouldin=2.260, Calinski-Harabasz=19.5\n",
      "k=6: Silhouette=0.094, Davies-Bouldin=2.155, Calinski-Harabasz=18.2\n",
      "k=7: Silhouette=0.097, Davies-Bouldin=2.230, Calinski-Harabasz=17.5\n",
      "k=8: Silhouette=0.065, Davies-Bouldin=2.398, Calinski-Harabasz=16.4\n",
      "k=9: Silhouette=0.058, Davies-Bouldin=2.117, Calinski-Harabasz=17.1\n",
      "k=10: Silhouette=0.090, Davies-Bouldin=2.021, Calinski-Harabasz=17.7\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3. DATA PREPROCESSING\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "#df = df.drop(columns=[\"encounter_id\", \"patient_nbr\", \"weight\", \"payer_code\", \"medical_specialty\"], axis=1)\n",
    "\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# For numeric columns: convert safely and fill with mean\n",
    "for col in df.select_dtypes(exclude='object').columns:\n",
    "    df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "print(\"Total missing values:\", df.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# CRITICAL FIX: Only drop rows with missing values in the SELECTED FEATURES\n",
    "# Don't drop based on columns we're not using (like medical_specialty, payer_code, etc.)\n",
    "features_to_check = numeric_features + categorical_features + medication_features\n",
    "df_cluster = df_cluster.dropna(subset=features_to_check)\n",
    "\n",
    "#print(f\"\\nDropped {initial_rows - len(df_cluster)} rows with missing values in selected features\")\n",
    "print(f\"Final dataset size: {df_cluster.shape}\")\n",
    "#print(f\"Retention rate: {len(df_cluster)/initial_rows*100:.1f}%\")\n",
    "\n",
    "# Separate features\n",
    "X_numeric = df_cluster[numeric_features].copy()\n",
    "X_categorical = df_cluster[categorical_features].copy()\n",
    "X_medications = df_cluster[medication_features].copy()\n",
    "\n",
    "# =========================\n",
    "# 4. ENCODE CATEGORICAL FEATURES\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENCODING FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Label encoding\n",
    "X_categorical_encoded = X_categorical.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X_categorical_encoded[col] = le.fit_transform(X_categorical[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "X_medications_encoded = X_medications.copy()\n",
    "for col in medication_features:\n",
    "    le = LabelEncoder()\n",
    "    X_medications_encoded[col] = le.fit_transform(X_medications[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Combine all features\n",
    "X_combined = pd.concat([X_numeric, X_categorical_encoded, X_medications_encoded], axis=1)\n",
    "print(f\"Combined feature matrix: {X_combined.shape}\")\n",
    "\n",
    "# =========================\n",
    "# 5. SCALE FEATURES\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCALING FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_combined)\n",
    "print(f\"Scaled data shape: {X_scaled.shape}\")\n",
    "\n",
    "# =========================\n",
    "# 6. FIND OPTIMAL NUMBER OF CLUSTERS\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINDING OPTIMAL NUMBER OF CLUSTERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "calinski_scores = []\n",
    "\n",
    "print(\"\\nEvaluating different k values...\")\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, labels))\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(X_scaled, labels))\n",
    "    calinski_scores.append(calinski_harabasz_score(X_scaled, labels))\n",
    "    \n",
    "    print(f\"k={k}: Silhouette={silhouette_scores[-1]:.3f}, \"\n",
    "          f\"Davies-Bouldin={davies_bouldin_scores[-1]:.3f}, \"\n",
    "          f\"Calinski-Harabasz={calinski_scores[-1]:.1f}\")\n",
    "\n",
    "# Plot evaluation metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0, 0].plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Inertia', fontsize=12)\n",
    "axes[0, 0].set_title('Elbow Method', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette score\n",
    "axes[0, 1].plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[0, 1].set_title('Silhouette Analysis', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Davies-Bouldin index\n",
    "axes[1, 0].plot(k_range, davies_bouldin_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Davies-Bouldin Index', fontsize=12)\n",
    "axes[1, 0].set_title('Davies-Bouldin Index (Lower is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Calinski-Harabasz score\n",
    "axes[1, 1].plot(k_range, calinski_scores, 'mo-', linewidth=2, markersize=8)\n",
    "axes[1, 1].set_xlabel('Number of Clusters (k)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Calinski-Harabasz Score', fontsize=12)\n",
    "axes[1, 1].set_title('Calinski-Harabasz Score', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine optimal k\n",
    "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nâœ… Optimal k (by silhouette score): {optimal_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7. APPLY K-MEANS CLUSTERING\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"APPLYING K-MEANS (k={optimal_k})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df_cluster['kmeans_cluster'] = kmeans_labels\n",
    "\n",
    "print(f\"\\nK-Means cluster distribution:\")\n",
    "print(df_cluster['kmeans_cluster'].value_counts().sort_index())\n",
    "print(f\"\\nCluster percentages:\")\n",
    "print(df_cluster['kmeans_cluster'].value_counts(normalize=True).sort_index() * 100)\n",
    "\n",
    "# =========================\n",
    "# 8. APPLY HIERARCHICAL CLUSTERING\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"APPLYING HIERARCHICAL CLUSTERING (k={optimal_k})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try different linkage methods\n",
    "linkage_methods = ['ward', 'complete', 'average']\n",
    "hierarchical_results = {}\n",
    "\n",
    "for method in linkage_methods:\n",
    "    hier = AgglomerativeClustering(n_clusters=optimal_k, linkage=method)\n",
    "    hier_labels = hier.fit_predict(X_scaled)\n",
    "    silhouette = silhouette_score(X_scaled, hier_labels)\n",
    "    \n",
    "    hierarchical_results[method] = {\n",
    "        'labels': hier_labels,\n",
    "        'silhouette': silhouette\n",
    "    }\n",
    "    \n",
    "    print(f\"Linkage: {method}, Silhouette: {silhouette:.3f}\")\n",
    "\n",
    "# Use best method\n",
    "best_linkage = max(hierarchical_results, key=lambda x: hierarchical_results[x]['silhouette'])\n",
    "hier_labels = hierarchical_results[best_linkage]['labels']\n",
    "\n",
    "df_cluster['hierarchical_cluster'] = hier_labels\n",
    "\n",
    "print(f\"\\nâœ… Best linkage method: {best_linkage}\")\n",
    "print(f\"\\nHierarchical cluster distribution:\")\n",
    "print(df_cluster['hierarchical_cluster'].value_counts().sort_index())\n",
    "\n",
    "# Create dendrogram (on sample for speed)\n",
    "print(\"\\nCreating dendrogram...\")\n",
    "sample_size = min(1000, len(X_scaled))\n",
    "sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
    "X_sample = X_scaled[sample_indices]\n",
    "\n",
    "linkage_matrix = linkage(X_sample, method=best_linkage)\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "dendrogram(linkage_matrix, truncate_mode='lastp', p=30)\n",
    "plt.xlabel('Sample Index or (Cluster Size)', fontsize=12)\n",
    "plt.ylabel('Distance', fontsize=12)\n",
    "plt.title(f'Hierarchical Clustering Dendrogram ({best_linkage} linkage)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# 9. APPLY DBSCAN\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"APPLYING DBSCAN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test different eps values\n",
    "eps_values = [0.5, 1.0, 1.5, 2.0]\n",
    "dbscan_results = {}\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=5, n_jobs=-1)\n",
    "    dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "    \n",
    "    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "    n_noise = np.sum(dbscan_labels == -1)\n",
    "    \n",
    "    dbscan_results[eps] = {\n",
    "        'labels': dbscan_labels,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'pct_noise': n_noise / len(dbscan_labels) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"eps={eps}: {n_clusters} clusters, {n_noise} noise points ({n_noise/len(dbscan_labels)*100:.1f}%)\")\n",
    "\n",
    "# Choose eps with reasonable noise level (5-10%)\n",
    "optimal_eps = 1.5\n",
    "dbscan_labels = dbscan_results[optimal_eps]['labels']\n",
    "\n",
    "df_cluster['dbscan_cluster'] = dbscan_labels\n",
    "\n",
    "print(f\"\\nâœ… Using eps={optimal_eps}\")\n",
    "print(f\"Clusters: {dbscan_results[optimal_eps]['n_clusters']}\")\n",
    "print(f\"Noise points: {dbscan_results[optimal_eps]['n_noise']}\")\n",
    "\n",
    "# =========================\n",
    "# 10. PCA DIMENSIONALITY REDUCTION\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PCA DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Full PCA to see variance explained\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1), \n",
    "         pca_full.explained_variance_ratio_, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Principal Component', fontsize=12)\n",
    "plt.ylabel('Explained Variance Ratio', fontsize=12)\n",
    "plt.title('PCA Scree Plot', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "plt.plot(range(1, len(cumsum) + 1), cumsum, 'ro-', linewidth=2, markersize=8)\n",
    "plt.axhline(y=0.95, color='green', linestyle='--', label='95% variance')\n",
    "plt.xlabel('Number of Components', fontsize=12)\n",
    "plt.ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "plt.title('Cumulative Explained Variance', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumsum >= 0.95) + 1\n",
    "print(f\"\\nComponents needed for 95% variance: {n_components_95}\")\n",
    "print(f\"Top 10 components explain: {cumsum[9]:.2%} of variance\")\n",
    "\n",
    "# Apply PCA for visualization (2D and 3D)\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "df_cluster['pca_1'] = X_pca_2d[:, 0]\n",
    "df_cluster['pca_2'] = X_pca_2d[:, 1]\n",
    "\n",
    "print(f\"\\n2D PCA explains: {pca_2d.explained_variance_ratio_.sum():.2%} of variance\")\n",
    "print(f\"3D PCA explains: {pca_3d.explained_variance_ratio_.sum():.2%} of variance\")\n",
    "\n",
    "# =========================\n",
    "# 11. t-SNE DIMENSIONALITY REDUCTION\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"t-SNE DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# t-SNE on sample (it's slow on large datasets)\n",
    "sample_size_tsne = min(10000, len(X_scaled))\n",
    "sample_indices_tsne = np.random.choice(len(X_scaled), sample_size_tsne, replace=False)\n",
    "X_sample_tsne = X_scaled[sample_indices_tsne]\n",
    "kmeans_sample = kmeans_labels[sample_indices_tsne]\n",
    "\n",
    "print(f\"Running t-SNE on {sample_size_tsne} samples...\")\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_sample_tsne)\n",
    "\n",
    "print(\"âœ… t-SNE complete\")\n",
    "\n",
    "# =========================\n",
    "# 12. UMAP DIMENSIONALITY REDUCTION (Optional)\n",
    "# =========================\n",
    "if UMAP_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"UMAP DIMENSIONALITY REDUCTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Running UMAP on {sample_size_tsne} samples...\")\n",
    "    \n",
    "    umap = UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "    X_umap = umap.fit_transform(X_sample_tsne)\n",
    "    \n",
    "    print(\"âœ… UMAP complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a78cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 13. VISUALIZE CLUSTERS WITH DIFFERENT METHODS\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUALIZING CLUSTERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PCA visualization with all three clustering methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# K-Means\n",
    "scatter1 = axes[0].scatter(df_cluster['pca_1'], df_cluster['pca_2'], \n",
    "                           c=df_cluster['kmeans_cluster'], \n",
    "                           cmap='tab10', alpha=0.6, s=20,\n",
    "                           edgecolors='black', linewidth=0.3)\n",
    "axes[0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "axes[0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "axes[0].set_title(f'K-Means Clustering (k={optimal_k})', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Hierarchical\n",
    "scatter2 = axes[1].scatter(df_cluster['pca_1'], df_cluster['pca_2'], \n",
    "                           c=df_cluster['hierarchical_cluster'], \n",
    "                           cmap='tab10', alpha=0.6, s=20,\n",
    "                           edgecolors='black', linewidth=0.3)\n",
    "axes[1].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "axes[1].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "axes[1].set_title(f'Hierarchical Clustering ({best_linkage})', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "# DBSCAN\n",
    "scatter3 = axes[2].scatter(df_cluster['pca_1'], df_cluster['pca_2'], \n",
    "                           c=df_cluster['dbscan_cluster'], \n",
    "                           cmap='tab10', alpha=0.6, s=20,\n",
    "                           edgecolors='black', linewidth=0.3)\n",
    "axes[2].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "axes[2].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "axes[2].set_title(f'DBSCAN (eps={optimal_eps})', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(scatter3, ax=axes[2], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# 14. COMPARE DIMENSIONALITY REDUCTION METHODS\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARING DIMENSIONALITY REDUCTION METHODS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_plots = 3 if UMAP_AVAILABLE else 2\n",
    "fig, axes = plt.subplots(1, n_plots, figsize=(6*n_plots, 5))\n",
    "\n",
    "if n_plots == 2:\n",
    "    axes = [axes[0], axes[1]]\n",
    "\n",
    "# PCA\n",
    "scatter1 = axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], \n",
    "                           c=kmeans_labels, cmap='tab10', alpha=0.5, s=10)\n",
    "axes[0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "axes[0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "axes[0].set_title('PCA', fontsize=13, fontweight='bold')\n",
    "\n",
    "# t-SNE\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                           c=kmeans_sample, cmap='tab10', alpha=0.5, s=10)\n",
    "axes[1].set_xlabel('t-SNE 1', fontsize=11)\n",
    "axes[1].set_ylabel('t-SNE 2', fontsize=11)\n",
    "axes[1].set_title(f't-SNE (n={sample_size_tsne})', fontsize=13, fontweight='bold')\n",
    "\n",
    "# UMAP\n",
    "if UMAP_AVAILABLE:\n",
    "    scatter3 = axes[2].scatter(X_umap[:, 0], X_umap[:, 1], \n",
    "                               c=kmeans_sample, cmap='tab10', alpha=0.5, s=10)\n",
    "    axes[2].set_xlabel('UMAP 1', fontsize=11)\n",
    "    axes[2].set_ylabel('UMAP 2', fontsize=11)\n",
    "    axes[2].set_title(f'UMAP (n={sample_size_tsne})', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# 15. 3D PCA VISUALIZATION\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3D PCA VISUALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2],\n",
    "                     c=kmeans_labels, cmap='tab10', alpha=0.5, s=10)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "ax.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "ax.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.1%})', fontsize=11)\n",
    "ax.set_title('3D PCA Visualization of K-Means Clusters', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster', pad=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# 16. CLUSTER PROFILING\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLUSTER PROFILING (K-MEANS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df_cluster[df_cluster['kmeans_cluster'] == cluster_id]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTER {cluster_id} (n={len(cluster_data)}, {len(cluster_data)/len(df_cluster)*100:.1f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Numeric features\n",
    "    print(\"\\nNumeric Features:\")\n",
    "    for feat in numeric_features:\n",
    "        mean_val = cluster_data[feat].mean()\n",
    "        median_val = cluster_data[feat].median()\n",
    "        print(f\"  {feat}: mean={mean_val:.2f}, median={median_val:.2f}\")\n",
    "    \n",
    "    # Readmission distribution\n",
    "    print(\"\\nReadmission Distribution:\")\n",
    "    readmit_dist = cluster_data['readmitted'].value_counts()\n",
    "    for cat, count in readmit_dist.items():\n",
    "        print(f\"  {cat}: {count} ({count/len(cluster_data)*100:.1f}%)\")\n",
    "\n",
    "# =========================\n",
    "# 17. CLUSTER COMPARISON HEATMAP\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING CLUSTER HEATMAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate cluster centers\n",
    "cluster_centers = pd.DataFrame()\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = df_cluster[df_cluster['kmeans_cluster'] == cluster_id]\n",
    "    cluster_means = cluster_data[numeric_features].mean()\n",
    "    cluster_centers[f'Cluster {cluster_id}'] = cluster_means\n",
    "\n",
    "# Standardize for heatmap\n",
    "cluster_centers_std = (cluster_centers - cluster_centers.mean(axis=1).values.reshape(-1, 1)) / cluster_centers.std(axis=1).values.reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cluster_centers_std, annot=True, fmt='.2f', cmap='RdYlGn', \n",
    "            center=0, cbar_kws={'label': 'Standardized Value'},\n",
    "            linewidths=0.5)\n",
    "plt.xlabel('Cluster', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Cluster Centers Heatmap (Standardized)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# 18. SILHOUETTE ANALYSIS\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SILHOUETTE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "silhouette_vals = silhouette_samples(X_scaled, kmeans_labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(optimal_k):\n",
    "    cluster_silhouette_vals = silhouette_vals[kmeans_labels == i]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    \n",
    "    size_cluster_i = cluster_silhouette_vals.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    \n",
    "    color = plt.cm.tab10(i / optimal_k)\n",
    "    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette_vals,\n",
    "                     facecolor=color, edgecolor=color, alpha=0.7)\n",
    "    \n",
    "    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "ax.set_xlabel('Silhouette Coefficient', fontsize=12)\n",
    "ax.set_ylabel('Cluster', fontsize=12)\n",
    "ax.set_title('Silhouette Analysis for K-Means Clustering', fontsize=14, fontweight='bold')\n",
    "\n",
    "avg_silhouette = silhouette_score(X_scaled, kmeans_labels)\n",
    "ax.axvline(x=avg_silhouette, color=\"red\", linestyle=\"--\", \n",
    "           label=f'Average Score: {avg_silhouette:.3f}')\n",
    "ax.legend()\n",
    "ax.set_yticks([])\n",
    "ax.set_xlim([-0.1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Silhouette Score: {avg_silhouette:.3f}\")\n",
    "\n",
    "# =========================\n",
    "# 19. SAVE RESULTS\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save clustered data\n",
    "# df_cluster.to_csv('data/processed/clustered_patients.csv', index=False)\n",
    "# print(\"âœ… Saved clustered data\")\n",
    "\n",
    "# Save cluster centers\n",
    "# cluster_centers.to_csv('results/tables/cluster_centers.csv')\n",
    "# print(\"âœ… Saved cluster centers\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ¨ CLUSTERING & DIMENSIONALITY REDUCTION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"  - Optimal k: {optimal_k}\")\n",
    "print(f\"  - K-Means Silhouette: {silhouette_score(X_scaled, kmeans_labels):.3f}\")\n",
    "print(f\"  - Hierarchical Silhouette: {silhouette_score(X_scaled, hier_labels):.3f}\")\n",
    "print(f\"  - DBSCAN clusters: {dbscan_results[optimal_eps]['n_clusters']}\")\n",
    "print(f\"  - PCA (2D) variance: {pca_2d.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"  - Components for 95% variance: {n_components_95}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
